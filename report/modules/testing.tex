\section{Testing}

Ensuring the reliability and correctness of the logic simulator was a critical aspect of the development process. Given the educational nature of the application—where students rely on the game to understand complex computer architecture concepts—it was imperative that the underlying logic gates and processor components behaved exactly as expected. Our testing strategy employed a combination of automated unit and integration tests, continuous integration (CI) pipelines, and manual gameplay verification.

\subsection{Automated Testing Framework}
The core of our verification strategy relied on automated testing using the \textbf{pytest} framework. This allowed us to rigorously verify individual components in isolation as well as their interactions within the application.

\begin{itemize}
    \item \textbf{Unit Testing (Model Layer):} The most critical tests focused on the \texttt{src.model} package. Since this layer contains the logic for digital components (e.g., \texttt{And}, \texttt{Or}, \texttt{ALUAdvanced}, \texttt{InstructionMemory}), we implemented extensive unit tests to verify their truth tables and state transitions. For example, the \texttt{ALUAdvanced} component was tested against a wide range of inputs, including edge cases for overflow, zero-flag generation, and signal inversion~\cite{test_alu}.
    
    \item \textbf{GUI and Integration Testing (View \& Control Layers):} To test the user interface and the interaction between controllers and the view, we utilized \textbf{pytest-qt}. This extension allowed us to simulate user interactions (such as mouse clicks and signal events) within the PySide6 environment without launching the full application. We also employed \texttt{unittest.mock} to isolate the \texttt{LevelScene} from the backend logic, enabling us to test the visual feedback mechanisms—such as the "Success" or "Failure" dialogs—deterministically~\cite{test_levelscene}.
\end{itemize}

\subsection{Test Coverage and Methodology}
We adopted a development methodology where tests were implemented immediately following the feature development. This approach allowed us to verify that new code met the functional requirements and helped prevent regression bugs as the complexity of the processor model grew.

To quantify the effectiveness of our testing, we utilized the \textbf{pytest-cov} plugin. Our automated test suite achieved a code coverage of \textbf{93\%}, indicating that the vast majority of our codebase, including edge cases in the logic gates and controller paths, is actively verified by our test suite.

\subsection{Continuous Integration (CI)}
To maintain code quality within our collaborative workflow, we implemented a Continuous Integration pipeline using \textbf{GitHub Actions}. We defined a "Quality Gate" workflow that triggers automatically on every Pull Request to the \texttt{main} branch. This workflow sets up a Python 3.13 environment, installs system dependencies (such as \texttt{libgl1-mesa-dev} for the GUI framework), and executes the full test suite~\cite{quality_gate}. This ensured that no broken code could be merged into the production branch, significantly reducing integration issues during the project.
